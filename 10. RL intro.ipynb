{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run following command to install deps:\n",
    "\n",
    "```pip install tensorflow==1.13.1 keras keras-rl gym```\n",
    "\n",
    "If you are interested to head into deeper details, look into - [Deep RL](https://github.com/trokas/Deep_RL), which contains more examples and intuitive lower level implementations. This [medium](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724) series is great. Also you can look into good book - Deep Reinforcement Learning Hands-On by Maxim Laptan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Let's start by looking at markov decision process.\n",
    "\n",
    "<img src=\"img/markov_decision_process.png\" alt=\"Markov decision process\" style=\"width: 600px;\"/>\n",
    "\n",
    "This can be represented with transition weights as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [ # shape=[s, a, s']\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None]]\n",
    "rewards = [ # shape=[s, a, s']\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to run through iterativ optimization process\n",
    "\n",
    "$$Q_{k+1} (s,a) \\leftarrow \\sum_{s'} T(s,a,s') [R(s,a,s') + \\gamma \\max_{a'} Q_k(s', a')] \\; \\text{for all} \\; (s'a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0  # for all possible actions\n",
    "    \n",
    "gamma = 0.90 # the discount factor\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                    transition_probabilities[s][a][sp]\n",
    "                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.91891892, 17.02702702, 13.62162162],\n",
       "       [ 0.        ,        -inf, -4.87971488],\n",
       "       [       -inf, 50.13365013,        -inf]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of using discounted rewards in Q-states is one of the fundamental ideas in RL. For sure we don't know initial probabilities and rewards, but as we will see we can learn them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole and DQN\n",
    "\n",
    "Get the environment and extract the number of actions.\n",
    "\n",
    "We will try to balance a stick - [CartPole](https://github.com/openai/gym/wiki/CartPole-v0)\n",
    "\n",
    "![](https://miro.medium.com/max/960/1*G_whtIrY9fGlw3It6HFfhA.gif)\n",
    "\n",
    "To meet provide this challenge we are going to utilize the [OpenAI gym](https://gym.openai.com/docs/), a collection of reinforcement learning environments.\n",
    "\n",
    "- Observations — The agent needs to know where pole currently is, and the angle at which it is balancing.\n",
    "- Delayed reward — Keeping the pole in the air as long as possible means moving in ways that will be advantageous for both the present and the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "nb_actions = env.action_space.n\n",
    "print('Number of actions', nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we configure and compile our agent. We will use Epsilon Greedy:\n",
    "- All actions initially are tried with non-zero probability\n",
    "- With probability $1-\\epsilon$ choose the greedy action\n",
    "- With probability $\\epsilon$ choose an action ar random\n",
    "\n",
    "and we will estimate target Q-Value using reward and the future discounted value estimate\n",
    "\n",
    "$$Q_{target}(s,a) = r + \\gamma \\cdot \\max_{a'} Q_\\theta (s', a').$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, \n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it looks like before training. Note, that pole does not have to fall fully for gym to note it as a failed play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 71.000, steps: 71\n",
      "Episode 2: reward: 140.000, steps: 140\n",
      "Episode 3: reward: 40.000, steps: 40\n",
      "Episode 4: reward: 54.000, steps: 54\n",
      "Episode 5: reward: 66.000, steps: 66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x132f61410>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now it's time to learn something! You can visualize the training by setting `visualize=True`, but this\n",
    "slows down training quite a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trokas/.environments/ai_primer/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   27/5000: episode: 1, duration: 2.233s, episode steps: 27, steps per second: 12, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.111 [-0.189, 0.928], loss: 0.447221, mae: 0.503291, mean_q: 0.109952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trokas/.environments/ai_primer/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  113/5000: episode: 2, duration: 0.542s, episode steps: 86, steps per second: 159, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.201 [-1.454, 0.273], loss: 0.152574, mae: 0.524421, mean_q: 0.741481\n",
      "  126/5000: episode: 3, duration: 0.103s, episode steps: 13, steps per second: 126, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.112 [-0.764, 1.213], loss: 0.027968, mae: 0.693243, mean_q: 1.402542\n",
      "  139/5000: episode: 4, duration: 0.090s, episode steps: 13, steps per second: 144, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.105 [-0.950, 1.513], loss: 0.036448, mae: 0.729355, mean_q: 1.487590\n",
      "  148/5000: episode: 5, duration: 0.099s, episode steps: 9, steps per second: 91, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.135 [-1.379, 2.252], loss: 0.059996, mae: 0.842194, mean_q: 1.640448\n",
      "  156/5000: episode: 6, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.587, 2.538], loss: 0.067087, mae: 0.854875, mean_q: 1.689723\n",
      "  167/5000: episode: 7, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.118 [-0.949, 1.701], loss: 0.079707, mae: 0.917407, mean_q: 1.757794\n",
      "  177/5000: episode: 8, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.125 [-0.966, 1.527], loss: 0.066719, mae: 0.962448, mean_q: 1.846263\n",
      "  188/5000: episode: 9, duration: 0.079s, episode steps: 11, steps per second: 139, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.125 [-1.169, 1.902], loss: 0.072387, mae: 1.060721, mean_q: 2.027753\n",
      "  198/5000: episode: 10, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.141 [-1.727, 2.670], loss: 0.085987, mae: 1.070007, mean_q: 2.025749\n",
      "  210/5000: episode: 11, duration: 0.086s, episode steps: 12, steps per second: 140, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.098 [-1.592, 2.546], loss: 0.090487, mae: 1.124391, mean_q: 2.142149\n",
      "  219/5000: episode: 12, duration: 0.069s, episode steps: 9, steps per second: 130, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.756, 2.819], loss: 0.067740, mae: 1.193094, mean_q: 2.296983\n",
      "  228/5000: episode: 13, duration: 0.080s, episode steps: 9, steps per second: 112, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [-1.782, 2.869], loss: 0.090870, mae: 1.200993, mean_q: 2.331792\n",
      "  238/5000: episode: 14, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.143 [-1.584, 2.573], loss: 0.136439, mae: 1.286422, mean_q: 2.386233\n",
      "  253/5000: episode: 15, duration: 0.100s, episode steps: 15, steps per second: 150, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.133 [0.000, 1.000], mean observation: 0.089 [-2.187, 3.300], loss: 0.136064, mae: 1.328471, mean_q: 2.479613\n",
      "  263/5000: episode: 16, duration: 0.067s, episode steps: 10, steps per second: 148, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.141 [-1.564, 2.575], loss: 0.136815, mae: 1.381613, mean_q: 2.602669\n",
      "  273/5000: episode: 17, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.985, 3.118], loss: 0.169095, mae: 1.420226, mean_q: 2.646218\n",
      "  285/5000: episode: 18, duration: 0.120s, episode steps: 12, steps per second: 100, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.142 [-1.927, 3.108], loss: 0.183802, mae: 1.482862, mean_q: 2.793155\n",
      "  293/5000: episode: 19, duration: 0.055s, episode steps: 8, steps per second: 144, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.537, 2.513], loss: 0.225428, mae: 1.531985, mean_q: 2.766027\n",
      "  302/5000: episode: 20, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.143 [-1.326, 2.243], loss: 0.179435, mae: 1.541266, mean_q: 2.835691\n",
      "  310/5000: episode: 21, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [-1.558, 2.594], loss: 0.143616, mae: 1.613087, mean_q: 3.073483\n",
      "  322/5000: episode: 22, duration: 0.084s, episode steps: 12, steps per second: 143, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.109 [-1.376, 2.240], loss: 0.159155, mae: 1.628192, mean_q: 3.008483\n",
      "  331/5000: episode: 23, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.152 [-1.526, 2.477], loss: 0.153483, mae: 1.664401, mean_q: 3.139515\n",
      "  340/5000: episode: 24, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.143 [-1.604, 2.480], loss: 0.190309, mae: 1.678119, mean_q: 3.151763\n",
      "  350/5000: episode: 25, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.146 [-1.799, 2.779], loss: 0.175328, mae: 1.711804, mean_q: 3.222718\n",
      "  360/5000: episode: 26, duration: 0.071s, episode steps: 10, steps per second: 142, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [-1.965, 3.009], loss: 0.188575, mae: 1.780373, mean_q: 3.411561\n",
      "  368/5000: episode: 27, duration: 0.054s, episode steps: 8, steps per second: 147, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.602, 2.597], loss: 0.197092, mae: 1.746193, mean_q: 3.123680\n",
      "  377/5000: episode: 28, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.785, 2.813], loss: 0.230111, mae: 1.883529, mean_q: 3.602733\n",
      "  389/5000: episode: 29, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.084 [-1.990, 3.013], loss: 0.202572, mae: 1.785817, mean_q: 3.288256\n",
      "  399/5000: episode: 30, duration: 0.073s, episode steps: 10, steps per second: 138, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.998, 3.059], loss: 0.183290, mae: 1.881079, mean_q: 3.621368\n",
      "  408/5000: episode: 31, duration: 0.056s, episode steps: 9, steps per second: 160, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.164 [-1.722, 2.835], loss: 0.147113, mae: 1.852684, mean_q: 3.528851\n",
      "  417/5000: episode: 32, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.175 [-2.874, 1.753], loss: 0.190843, mae: 1.934857, mean_q: 3.698146\n",
      "  426/5000: episode: 33, duration: 0.077s, episode steps: 9, steps per second: 116, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.842, 1.754], loss: 0.172229, mae: 1.918566, mean_q: 3.621599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  437/5000: episode: 34, duration: 0.124s, episode steps: 11, steps per second: 89, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.150 [-1.746, 2.849], loss: 0.206125, mae: 1.956131, mean_q: 3.776603\n",
      "  446/5000: episode: 35, duration: 0.080s, episode steps: 9, steps per second: 113, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.772, 2.749], loss: 0.262391, mae: 1.958308, mean_q: 3.694489\n",
      "  456/5000: episode: 36, duration: 0.083s, episode steps: 10, steps per second: 120, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.121 [-1.585, 2.492], loss: 0.138729, mae: 2.035957, mean_q: 4.020289\n",
      "  466/5000: episode: 37, duration: 0.083s, episode steps: 10, steps per second: 121, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.143 [-2.623, 1.594], loss: 0.166527, mae: 2.070748, mean_q: 3.923180\n",
      "  475/5000: episode: 38, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.832, 1.759], loss: 0.156187, mae: 2.104419, mean_q: 4.035074\n",
      "  484/5000: episode: 39, duration: 0.060s, episode steps: 9, steps per second: 149, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.768, 2.873], loss: 0.121420, mae: 2.141403, mean_q: 4.135736\n",
      "  495/5000: episode: 40, duration: 0.070s, episode steps: 11, steps per second: 158, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.124 [-1.747, 2.761], loss: 0.177654, mae: 2.184344, mean_q: 4.201820\n",
      "  505/5000: episode: 41, duration: 0.070s, episode steps: 10, steps per second: 144, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.905, 3.012], loss: 0.188418, mae: 2.184389, mean_q: 4.197049\n",
      "  515/5000: episode: 42, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.155 [-1.721, 2.763], loss: 0.191144, mae: 2.221943, mean_q: 4.345866\n",
      "  529/5000: episode: 43, duration: 0.091s, episode steps: 14, steps per second: 154, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.071 [-1.400, 2.170], loss: 0.149738, mae: 2.240087, mean_q: 4.296639\n",
      "  556/5000: episode: 44, duration: 0.259s, episode steps: 27, steps per second: 104, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: 0.019 [-3.605, 2.858], loss: 0.183835, mae: 2.339338, mean_q: 4.392715\n",
      "  567/5000: episode: 45, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.148 [-2.827, 1.728], loss: 0.152491, mae: 2.410059, mean_q: 4.553226\n",
      "  576/5000: episode: 46, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.831, 1.711], loss: 0.250793, mae: 2.435319, mean_q: 4.571538\n",
      "  603/5000: episode: 47, duration: 0.168s, episode steps: 27, steps per second: 161, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.038 [-1.527, 2.093], loss: 0.173562, mae: 2.499212, mean_q: 4.729779\n",
      "  612/5000: episode: 48, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.822, 1.717], loss: 0.125334, mae: 2.559337, mean_q: 4.903101\n",
      "  621/5000: episode: 49, duration: 0.062s, episode steps: 9, steps per second: 146, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.814, 1.781], loss: 0.094079, mae: 2.556881, mean_q: 4.934499\n",
      "  668/5000: episode: 50, duration: 0.355s, episode steps: 47, steps per second: 132, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: -0.159 [-2.208, 0.996], loss: 0.231324, mae: 2.675764, mean_q: 5.063996\n",
      "  681/5000: episode: 51, duration: 0.088s, episode steps: 13, steps per second: 147, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.082 [-1.482, 0.978], loss: 0.180093, mae: 2.786591, mean_q: 5.347095\n",
      "  724/5000: episode: 52, duration: 0.286s, episode steps: 43, steps per second: 150, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.034 [-1.624, 1.497], loss: 0.219751, mae: 2.883540, mean_q: 5.503663\n",
      "  743/5000: episode: 53, duration: 0.116s, episode steps: 19, steps per second: 163, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.080 [-1.394, 0.808], loss: 0.221091, mae: 2.981159, mean_q: 5.720631\n",
      "  814/5000: episode: 54, duration: 0.455s, episode steps: 71, steps per second: 156, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.094 [-0.400, 0.931], loss: 0.327570, mae: 3.149784, mean_q: 5.977691\n",
      "  846/5000: episode: 55, duration: 0.286s, episode steps: 32, steps per second: 112, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.108 [-0.764, 0.229], loss: 0.299276, mae: 3.381050, mean_q: 6.486714\n",
      "  873/5000: episode: 56, duration: 0.166s, episode steps: 27, steps per second: 163, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.075 [-0.382, 0.718], loss: 0.258737, mae: 3.420804, mean_q: 6.594128\n",
      "  903/5000: episode: 57, duration: 0.198s, episode steps: 30, steps per second: 151, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.138 [-0.348, 0.721], loss: 0.404383, mae: 3.568400, mean_q: 6.839048\n",
      "  939/5000: episode: 58, duration: 0.224s, episode steps: 36, steps per second: 160, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.095 [-0.353, 0.881], loss: 0.476896, mae: 3.678858, mean_q: 7.014674\n",
      "  975/5000: episode: 59, duration: 0.230s, episode steps: 36, steps per second: 156, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.084 [-0.378, 0.818], loss: 0.417292, mae: 3.788357, mean_q: 7.293509\n",
      " 1045/5000: episode: 60, duration: 0.487s, episode steps: 70, steps per second: 144, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.056 [-0.358, 0.852], loss: 0.411049, mae: 3.962259, mean_q: 7.628405\n",
      " 1096/5000: episode: 61, duration: 0.322s, episode steps: 51, steps per second: 159, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.137 [-0.778, 0.328], loss: 0.413846, mae: 4.207229, mean_q: 8.151596\n",
      " 1145/5000: episode: 62, duration: 0.356s, episode steps: 49, steps per second: 137, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.114 [-0.342, 0.667], loss: 0.457262, mae: 4.399938, mean_q: 8.537928\n",
      " 1197/5000: episode: 63, duration: 0.338s, episode steps: 52, steps per second: 154, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.155 [-1.068, 0.530], loss: 0.476315, mae: 4.595942, mean_q: 8.928305\n",
      " 1227/5000: episode: 64, duration: 0.209s, episode steps: 30, steps per second: 144, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.413, 1.147], loss: 0.477106, mae: 4.773460, mean_q: 9.285989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1269/5000: episode: 65, duration: 0.285s, episode steps: 42, steps per second: 147, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.097 [-0.411, 0.810], loss: 0.479696, mae: 4.827694, mean_q: 9.430436\n",
      " 1328/5000: episode: 66, duration: 0.400s, episode steps: 59, steps per second: 148, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.108 [-0.708, 0.269], loss: 0.637441, mae: 5.085500, mean_q: 9.896491\n",
      " 1376/5000: episode: 67, duration: 0.305s, episode steps: 48, steps per second: 157, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.131 [-0.945, 0.272], loss: 0.586007, mae: 5.303317, mean_q: 10.316116\n",
      " 1400/5000: episode: 68, duration: 0.164s, episode steps: 24, steps per second: 146, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.124 [-0.715, 0.163], loss: 0.830239, mae: 5.455429, mean_q: 10.588214\n",
      " 1456/5000: episode: 69, duration: 0.409s, episode steps: 56, steps per second: 137, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.090 [-0.922, 0.521], loss: 0.885883, mae: 5.573091, mean_q: 10.802896\n",
      " 1500/5000: episode: 70, duration: 0.295s, episode steps: 44, steps per second: 149, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.108 [-0.165, 0.836], loss: 0.518641, mae: 5.730984, mean_q: 11.246057\n",
      " 1556/5000: episode: 71, duration: 0.348s, episode steps: 56, steps per second: 161, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.108 [-0.271, 0.756], loss: 0.590854, mae: 5.911607, mean_q: 11.574946\n",
      " 1671/5000: episode: 72, duration: 0.758s, episode steps: 115, steps per second: 152, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.112 [-0.798, 0.515], loss: 0.724373, mae: 6.201490, mean_q: 12.130905\n",
      " 1738/5000: episode: 73, duration: 0.481s, episode steps: 67, steps per second: 139, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.114 [-0.425, 0.879], loss: 0.728036, mae: 6.506877, mean_q: 12.744250\n",
      " 1793/5000: episode: 74, duration: 0.328s, episode steps: 55, steps per second: 168, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.148 [-0.889, 0.216], loss: 0.752038, mae: 6.663988, mean_q: 13.095311\n",
      " 1897/5000: episode: 75, duration: 0.652s, episode steps: 104, steps per second: 159, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.096 [-0.476, 1.073], loss: 0.705633, mae: 6.974518, mean_q: 13.712493\n",
      " 2025/5000: episode: 76, duration: 0.786s, episode steps: 128, steps per second: 163, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.172 [-0.501, 1.481], loss: 0.690753, mae: 7.403542, mean_q: 14.662148\n",
      " 2219/5000: episode: 77, duration: 1.247s, episode steps: 194, steps per second: 156, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.175 [-0.556, 1.788], loss: 0.658011, mae: 7.954641, mean_q: 15.844619\n",
      " 2390/5000: episode: 78, duration: 1.156s, episode steps: 171, steps per second: 148, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.223 [-0.522, 1.837], loss: 0.733732, mae: 8.608893, mean_q: 17.252285\n",
      " 2565/5000: episode: 79, duration: 1.178s, episode steps: 175, steps per second: 149, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.220 [-0.550, 1.681], loss: 0.889233, mae: 9.292573, mean_q: 18.639191\n",
      " 2722/5000: episode: 80, duration: 1.015s, episode steps: 157, steps per second: 155, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.237 [-0.569, 1.665], loss: 0.858757, mae: 9.930684, mean_q: 19.888203\n",
      " 2897/5000: episode: 81, duration: 1.142s, episode steps: 175, steps per second: 153, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.147 [-1.099, 0.563], loss: 0.920268, mae: 10.517445, mean_q: 21.187149\n",
      " 3097/5000: episode: 82, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.159 [-0.597, 1.486], loss: 1.123269, mae: 11.209455, mean_q: 22.544014\n",
      " 3278/5000: episode: 83, duration: 1.137s, episode steps: 181, steps per second: 159, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.222 [-0.793, 1.843], loss: 0.979073, mae: 11.980976, mean_q: 24.092852\n",
      " 3478/5000: episode: 84, duration: 1.286s, episode steps: 200, steps per second: 155, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.047 [-0.565, 0.501], loss: 1.040847, mae: 12.679613, mean_q: 25.549654\n",
      " 3678/5000: episode: 85, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.123 [-0.524, 1.132], loss: 1.130416, mae: 13.330642, mean_q: 26.881311\n",
      " 3838/5000: episode: 86, duration: 1.011s, episode steps: 160, steps per second: 158, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.272 [-1.085, 2.198], loss: 1.321319, mae: 14.001651, mean_q: 28.242390\n",
      " 3983/5000: episode: 87, duration: 0.891s, episode steps: 145, steps per second: 163, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.170 [-0.996, 0.536], loss: 1.269233, mae: 14.572875, mean_q: 29.448700\n",
      " 4183/5000: episode: 88, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.185 [-0.677, 1.803], loss: 1.235773, mae: 15.114050, mean_q: 30.527964\n",
      " 4366/5000: episode: 89, duration: 1.152s, episode steps: 183, steps per second: 159, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.304 [-0.822, 2.379], loss: 1.144327, mae: 15.820825, mean_q: 31.992817\n",
      " 4526/5000: episode: 90, duration: 0.994s, episode steps: 160, steps per second: 161, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.223 [-1.488, 0.429], loss: 1.399480, mae: 16.300388, mean_q: 32.928020\n",
      " 4726/5000: episode: 91, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.203 [-0.743, 1.508], loss: 1.663439, mae: 16.891037, mean_q: 34.059887\n",
      " 4912/5000: episode: 92, duration: 1.222s, episode steps: 186, steps per second: 152, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.334 [-0.398, 2.118], loss: 1.460863, mae: 17.492495, mean_q: 35.393513\n",
      "done, took 34.947 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x133576d50>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=5000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our reinforcement learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 189.000, steps: 189\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 181.000, steps: 181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x12ea05050>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nearly a perfect play, since CartPole exits if 200 steps are reached. You can experiment with version which limit is 500 by changing env to `CartPole-v1`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
